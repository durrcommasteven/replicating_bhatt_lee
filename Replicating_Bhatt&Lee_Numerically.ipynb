{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZX4G3QT73FD"
   },
   "source": [
    "# Replicating Bhatt&Lee Numerically\n",
    "\n",
    "For conciseness, I import many of the functions which are used as 'helpers' here from Bhatt_Lee_tools.py\n",
    "\n",
    "I also test all of these in TEST_Bhatt_Lee_tools.py\n",
    "\n",
    "So if you maky any changes, they'll be tested automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24lFvYSS73FE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing tools for replicating Bhatt and Lee\n",
      "running checks on Bhatt_Lee_tools\n",
      "all tests successful\n"
     ]
    }
   ],
   "source": [
    "from Bhatt_Lee_tools import *\n",
    "import matplotlib.pyplot as plt \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM8yQuAf73FI"
   },
   "source": [
    "If one bond in a cluster of 4 is very strong, the lowest 4 states can be represented as a singlet/triplet with energies matching our eigenvalues.\n",
    "\n",
    "Given\n",
    "\n",
    "$$\n",
    "J/4 \\qquad -3J/4\n",
    "$$\n",
    "\n",
    "the spacing we expect to see for the singlet/triplet is \n",
    "\n",
    "$$\n",
    "J_{34} + (J_{13} -J_{23})(J_{24}-J_{14})/(2 J_{12}) + O(1/J_{12}^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3nfGdJke73FN"
   },
   "source": [
    "We can eliminate $J_{12}$ and write the whole cluster as two spins with coupling\n",
    "\n",
    "$$\n",
    "J'_{34} = J_{34} + (J_{13} - J_{23})(J_{24} - J_{14})/(2J_{12})\n",
    "$$\n",
    "\n",
    "Now we also need to modify couplings with other spins to obtain\n",
    "\n",
    "$$\n",
    "J'_{35} \\qquad J'_{45}\n",
    "$$\n",
    "\n",
    "We do this by fitting the eigenvalues of the reduced ham $\\sum_{i<j=1}^5 H_{ij}$ within the subspace consisting of the low lying singlet triplet set of (1,2,3,4) and the doublet on 5, to those of a 3 spin cluster (3', 4', 5)\n",
    "\n",
    "We can do this by simply getting the lowest 8 energies of the hamiltonian for (1,2,3,4) and 5\n",
    "\n",
    "this is the function: get_projected_energies(j12, j13, j14, j23, j24, j34, j51, j52, j53, j54)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afynmXrB73FU"
   },
   "source": [
    "## Fit to these eigenvalues\n",
    "\n",
    "Lets use these 8 energies to obtain\n",
    "\n",
    "$$\n",
    "J'_{35} \\qquad J'_{45}\n",
    "$$\n",
    "\n",
    "For the three spins (3, 4, 5) by matching eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eShN744AC8ms"
   },
   "source": [
    "The eigenvalues of three spins connected in such a way can be found. Here I indicate $\\times$ to indicate the multiplicity\n",
    "$$\n",
    "\\lambda_3 = \\frac{1}{4}(j12+j13+j23) \\times 4\n",
    "$$\n",
    "$$\n",
    "\\lambda_2 = \\frac{1}{4} \\left(2 \\sqrt{\\text{j12}^2-\\text{j23} (\\text{j12}+\\text{j13})-\\text{j12} \\text{j13}+\\text{j13}^2+\\text{j23}^2}-\\text{j12}-\\text{j13}-\\text{j23}\\right) \\times 2\n",
    "$$\n",
    "$$\n",
    "\\lambda_1 = \\frac{1}{4} \\left(-2 \\sqrt{\\text{j12}^2-\\text{j23} (\\text{j12}+\\text{j13})-\\text{j12} \\text{j13}+\\text{j13}^2+\\text{j23}^2}-\\text{j12}-\\text{j13}-\\text{j23}\\right) \\times 2\n",
    "$$\n",
    "\n",
    "this makes sense here since we have $2\\otimes 2\\otimes 2 = 4\\oplus 2\\oplus 2$\n",
    "\n",
    "We'll first identify the levels (identify_levels(eigenvalues))\n",
    "\n",
    "Then we'll fit values to these eigenvalues given $j34$ (fit_to_values(eigenvalues, j34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can do this analytically. \n",
    "\n",
    "Suppose we identify three levels: $\\rho_1$, $\\rho_2$, $\\rho_3$, and we are given $j23 = \\gamma$\n",
    "\n",
    "Then we can matche the following:\n",
    "$$\n",
    "\\alpha \\equiv \\rho_2-\\rho_1 = \\lambda_2 - \\lambda_1 = \\sqrt{\\text{j12}^2-\\text{j12} (\\text{j13}+\\text{j23})+\\text{j13}^2-\\text{j13} \\text{j23}+\\text{j23}^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta \\equiv \\rho_3-\\rho_1 =  \\lambda_3 - \\lambda_1 =\\frac{1}{2} \\left(\\sqrt{\\text{j12}^2-\\text{j23} (\\text{j12}+\\text{j13})-\\text{j12} \\text{j13}+\\text{j13}^2+\\text{j23}^2}+\\text{j12}+\\text{j13}+\\text{j23}\\right)\n",
    "$$\n",
    "Now I'll define \n",
    "$$\n",
    "\\alpha = \\sqrt{\\text{j12}^2-\\text{j23} (\\text{j12}+\\text{j13})-\\text{j12} \\text{j13}+\\text{j13}^2+\\text{j23}^2}\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "\\beta = \\frac{1}{2} \\left(\\alpha+\\text{j12}+\\text{j13}+\\gamma\\right)\n",
    "$$\n",
    "\n",
    "also we have\n",
    "\n",
    "$$\n",
    "\\delta \\equiv \\rho_3-\\rho_2 =  \\lambda_3 - \\lambda_2 = \\frac{1}{2} \\left(-\\sqrt{\\text{j12}^2-\\text{j23} (\\text{j12}+\\text{j13})-\\text{j12} \\text{j13}+\\text{j13}^2+\\text{j23}^2}+\\text{j12}+\\text{j13}+\\text{j23}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta = \\frac{1}{2} \\left(-\\alpha+\\text{j12}+\\text{j13}+\\gamma\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So \n",
    "$$\n",
    "\\beta + \\delta = j12 + j13 + \\gamma \\implies \\beta + \\delta - \\gamma = j12 + j13\n",
    "$$\n",
    "$$\n",
    "j12 = \\beta + \\delta - \\gamma - j13\n",
    "$$\n",
    "\n",
    "we can plug this into $\\alpha$ to get that \n",
    "\n",
    "$$\n",
    "\\alpha = \\sqrt{\n",
    "\\beta ^2-3 \\beta  \\gamma +2 \\beta  \\delta +3 \\gamma ^2-3 \\gamma  \\delta +\\delta ^2+3 \\text{j13}^2-3 \\text{j13} (\\beta -\\gamma +\\delta )\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have that\n",
    "$$\n",
    "\\beta - \\delta = \\alpha \\implies (\\beta-\\delta)^2 = \\beta ^2-3 \\beta  \\gamma +2 \\beta  \\delta +3 \\gamma ^2-3 \\gamma  \\delta +\\delta ^2+3 \\text{j13}^2-3 \\text{j13} (\\beta -\\gamma +\\delta )\n",
    "$$\n",
    "Solve for j13\n",
    "$$\n",
    "\\text{j13} = \\frac{1}{6} \\left(3 \\beta -3 \\gamma +3 \\delta\\pm \\sqrt{3} \\sqrt{3 \\beta ^2+6 \\beta  \\gamma -10 \\beta  \\delta -9 \\gamma ^2+6 \\gamma  \\delta +3 \\delta ^2} \\right)\n",
    "$$\n",
    "and so \n",
    "$$\n",
    "\\text{j12} = \\frac{1}{6} \\left(3 \\beta -3 \\gamma +3 \\delta\\mp \\sqrt{3} \\sqrt{3 \\beta ^2+6 \\beta  \\gamma -10 \\beta  \\delta -9 \\gamma ^2+6 \\gamma  \\delta +3 \\delta ^2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The functions:\n",
    "\n",
    "**get_energies(j12, j13, j14, j23, j24, j34)**: calculate numerically the lowest 4 eigenvalues of the spin cluster's hamiltonian. This is mainly for confirmation of what we expect to see.\n",
    "\n",
    "returns: lowest 4 eigenvalues of the (1,2,3,4) hamiltonian\n",
    "\n",
    "**get_projected_energies(j12, j13, j14, j23, j24, j34, j51, j52, j53, j54)**: numercially calculate the lowest 8 eigenvalues. These should be able to be mapped onto 3 spins later. \n",
    "\n",
    "returns: lowest 8 eigenvalues of the full (1,2,3,4) + 5 hamiltonian\n",
    "\n",
    "**identify_levels(eigenvalues)**: Identify the three corresponding clusters, given 8 eigenvalues. We expect to have a grouping of 4 eigenvalues, and two groupings of 2 eigenvalues. Give the means of each of these three clusters\n",
    "\n",
    "returns: [mean of 4-grouping, mean of 2-grouping, mean of 2-grouping]\n",
    "\n",
    "**fit_to_values(eigenvalues, j34)**: Now we combine the above functions here. Given 8 eigenvalues, we identify the correspoinding 3 clusters. Then we find the effective j35, j45, which give us those\n",
    "\n",
    "returns: [effective j35, effective j45]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These functions are used below\n",
    "renormalize_coupling takes coupling strengths between 1,2,3,4 and a fifth point 5\n",
    "\n",
    "returns the effective couplings j35 and j45 by fitting\n",
    "\"\"\"\n",
    "\n",
    "def renormalize_couplings(coupling_strengths):\n",
    "    \"\"\"\n",
    "    coupling_stengths is a list/tuple/array of the couplings of the cluster and th fifth point\n",
    "    in the order:\n",
    "    j12, j13, j14, j23, j24, j34, j51, j52, j53, j54\n",
    "    \n",
    "    return new couplings between p3 and p5 ,and p4 and p5\n",
    "    \"\"\"\n",
    "    j12, j13, j14, j23, j24, j34, j51, j52, j53, j54 = coupling_strengths\n",
    "    \n",
    "    eigenvalues = get_projected_energies(j12, j13, j14, j23, j24, j34, j51, j52, j53, j54)\n",
    "    \n",
    "    #FOR DEBUGGING\n",
    "    #we need to make sure that the levels we identify are correct\n",
    "    \n",
    "    print('eigenvalues:', eigenvalues)\n",
    "    v1, v2, v3 = split_up_eigenvalues(eigenvalues)\n",
    "    print(v1)\n",
    "    print(v2)\n",
    "    print(v3)\n",
    "    print(\"j12, j13, j14, j23, j24, j34, j51, j52, j53, j54\")\n",
    "    print(j12, j13, j14, j23, j24, j34, j51, j52, j53, j54)\n",
    "    print(\" \")\n",
    "    \n",
    "    \n",
    "    newj34 = j34 + (j13 - j23)*(j24 - j14)/(2*j12)\n",
    "    \n",
    "    j35, j45 = fit_to_values(eigenvalues, newj34)\n",
    "    \n",
    "    return j35, j45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DCqq2W8_D2WV"
   },
   "source": [
    "# Making the setup\n",
    "\n",
    "Now we spread particles throughout space and enact the rg procedure\n",
    "\n",
    "$$\n",
    "J(r) = J_0 \\exp(-2 r/a)\n",
    "$$\n",
    "with $n a^D \\ll 1$, we place 10000 randomly in a 2d plane. \n",
    "\n",
    "$$\n",
    "10000\\times a^2 \\ll 1 \\implies a \\ll 1/100\n",
    "$$\n",
    "\n",
    "let's take $a = (1/100)^2$\n",
    "\n",
    "We enforce a threshold, $J_c$, so that total interaction strengths less than the thresshold are removed. We should set $J_c$ so that there are initially \n",
    "15-30 couplings. \n",
    "\n",
    "We'll also set $J_0 =1$ without loss of generality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# space vs time tradeoff\n",
    "\n",
    "In order to deal with this large array, we make use of space. \n",
    "\n",
    "This means that we will make helper arrays which store information for us\n",
    "\n",
    "**points**: this is an array of all the points of shape (10000, 2)\n",
    "\n",
    "**point_dict**: this is a dictionary which tells us the connections to a point, and their coupling strength in order from strongest to weakest. In order to use this on a point, we need to convert the point to a tuple e.g. np.array([.5, .6]) -> (.5, .6) \n",
    "\n",
    "(since dictionary keys must be immutable, and tuples are immutable).\n",
    "\n",
    "E.g.: point_dict[(.1, .45)] = [((.105, .5), .7), ((.2, .1), .6),((.7, .9), .2)]\n",
    "\n",
    "This point is connected to 3 points, with couplings .7, .6, and .2\n",
    " \n",
    "$$ $$\n",
    "\n",
    "Also, in order to find these couplings in the first place, we use a discretization of space.\n",
    "\n",
    "\n",
    "**space_dict** is a dictionary which maps regions in space to the points in them. But it is only used in the definition of make_arrangement\n",
    "\n",
    "by using memory (making dictionaries), we convert the process of creating an arrangement of these points from one that is $O(N^2)$ to one that is $\\sim O(N)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_7JTnEe73FZ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define a function to initialize a setup\n",
    "This is included here in case modification is needed\n",
    "\"\"\"\n",
    "def make_arrangement(Jc=.01, a=(1/250), N = 10000):\n",
    "    points = np.random.rand(N, 2)\n",
    "    \n",
    "    #dictionary listing points close enough to count\n",
    "    #as well as their coupling strength: (point, coupling)\n",
    "    point_dict = dict()\n",
    "    \n",
    "    #space dictionary:\n",
    "    #because we have so many points, we'll make use of space \n",
    "    #and speed things up\n",
    "    \n",
    "    #dx should be the max distance that the cutoff allows\n",
    "    dx = -a*np.log(Jc)\n",
    "    \n",
    "    #a dictionary to hold the points in a region\n",
    "    #then to find connections, we'll just look at the neighboring boxes: (x \\pm dx, y \\pm dx)\n",
    "    #To avoid machine precision errors, we'll round to 8 decimal places here\n",
    "    \n",
    "    num_slices = len(np.arange(0, 1, dx))\n",
    "    space_dict = {(x, y): [] for x in range(num_slices) for y in range(num_slices)}\n",
    "    \n",
    "    def map_to_point(x, y):\n",
    "        \"\"\"\n",
    "        given a point in our region, what is the corresponding block\n",
    "        in space_dict\n",
    "        \"\"\"\n",
    "        return int(x/(dx)), int(y/(dx))\n",
    "    \n",
    "    for p in points:\n",
    "        coresponding_i, corresponding_j = map_to_point(p[0], p[1])\n",
    "        space_dict[(coresponding_i, corresponding_j)].append(p)\n",
    "    \n",
    "    point_dict = {(p[0], p[1]) : [] for p in points}\n",
    "    \n",
    "    for i, p in enumerate(points):\n",
    "        tuple_p = (p[0], p[1])\n",
    "        #point_dict[tuple_p] = []\n",
    "        \n",
    "        \"\"\"\n",
    "        Construct a list of points to check\n",
    "        \"\"\"\n",
    "        close_points = []\n",
    "        x, y = map_to_point(p[0], p[1])\n",
    "        steps = [(x,y), (x, y+1), (x+1, y), (x, y-1), (x-1, y),\n",
    "                 (x+1, y+1), (x+1, y-1), (x-1, y+1), (x-1, y-1)]\n",
    "        \n",
    "        #periodic bcs\n",
    "        steps = map(lambda x: (x[0] % num_slices, x[1] % num_slices), steps)\n",
    "        steps = list(set(steps))\n",
    "        \n",
    "        for step in steps:\n",
    "            if step in space_dict:\n",
    "                close_points+=space_dict[step]\n",
    "        \n",
    "        #take note of points we've already included\n",
    "        if point_dict[tuple_p]:\n",
    "            prev_points, _ = zip(*point_dict[tuple_p]) \n",
    "        else:\n",
    "            prev_points = []\n",
    "        \n",
    "        for j, p2 in enumerate(close_points):\n",
    "            #check that it hasn't already been included\n",
    "            if tuple(p2) not in prev_points:\n",
    "                #check that its above the threshold\n",
    "                #since we have periodic bc's we need to consider \n",
    "                #shifted p2's \n",
    "\n",
    "                #are we close enough to care?\n",
    "                #are we within dx of the boundary\n",
    "                if min(p2[0], 1-p2[0])<=dx or min(p2[1], 1-p2[1])<=dx:\n",
    "                    #we're close enough to the boundary for periodic bc's to matter\n",
    "                    radii = []\n",
    "                    for xstep in [-1, 0, 1]:\n",
    "                        for ystep in [-1, 0, 1]:\n",
    "                            shifted_point = (p2[0]+xstep, p2[1]+ystep)\n",
    "                            radii.append(LA.norm(p-shifted_point))\n",
    "\n",
    "                    r = np.min(radii)\n",
    "                else:\n",
    "                    r = LA.norm((p[0]-p2[0], p[1]-p2[1]))\n",
    "\n",
    "                coupling = np.exp(-2*r/a)\n",
    "\n",
    "                if coupling>=Jc and r!=0:\n",
    "                    #add it to dictionary\n",
    "                    point_dict[tuple_p].append((tuple(p2), coupling))\n",
    "                    point_dict[tuple(p2)].append((tuple_p, coupling))\n",
    "        \n",
    "        #now sort by coupling strength\n",
    "        #with the strongest couplings coming first\n",
    "        point_dict[tuple_p].sort(key = lambda x: -x[-1])\n",
    "    \n",
    "\n",
    "    #now return the points and the dictionary \n",
    "    return points, point_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test make_arrangement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22756\\999748165.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mystep\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                         \u001b[0mshifted_point\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mxstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mystep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                         \u001b[0mradii\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mshifted_point\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mshifted_point\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mradii\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2528\u001b[0m                 \u001b[0msqnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2530\u001b[1;33m                 \u001b[0msqnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2531\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2532\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now we test this on a small enough N \n",
    "So that we can compute everything manually, and make sure our\n",
    "faster version agrees\n",
    "\"\"\"\n",
    "test_Jc = .01\n",
    "test_a = 1/110\n",
    "test_N=100\n",
    "\n",
    "\"\"\"\n",
    "We'll run this a few times since randomness is involved to make sure \n",
    "it really does work\n",
    "\"\"\"\n",
    "for rep in range(60):\n",
    "    if rep%5==0:\n",
    "        print('test ',rep)\n",
    "    test_points, test_point_dict = make_arrangement(Jc=test_Jc, a=test_a, N = test_N)\n",
    "    \n",
    "    #cornfim that test_point_dict matches what it should be\n",
    "    confirmation_point_dict = {(p[0], p[1]) : [] for p in test_points}\n",
    "\n",
    "    for i in range(test_points.shape[0]):\n",
    "        p = (test_points[i][0], test_points[i][1])\n",
    "        \n",
    "        if confirmation_point_dict[p]:\n",
    "            prev_points, _ = zip(*confirmation_point_dict[p])\n",
    "        else:\n",
    "            prev_points = []\n",
    "        \n",
    "        for j in range(test_points.shape[0]):\n",
    "            if j != i and (test_points[j][0], test_points[j][1]) not in prev_points:\n",
    "                p2 = (test_points[j][0], test_points[j][1])\n",
    "                #compute the coupling\n",
    "                radii = []\n",
    "                for xstep in [-1, 0, 1]:\n",
    "                    for ystep in [-1, 0, 1]:\n",
    "                        shifted_point = (p2[0]+xstep, p2[1]+ystep)\n",
    "                        radii.append(LA.norm((p[0]-shifted_point[0], p[1]-shifted_point[1])))\n",
    "\n",
    "                r = np.min(radii)\n",
    "                coupling = np.exp(-2*r/(test_a))\n",
    "                \n",
    "                #is it greater than test_Jc\n",
    "                if coupling>=test_Jc:\n",
    "                    confirmation_point_dict[p].append((p2, coupling))\n",
    "                    confirmation_point_dict[p2].append((p, coupling))\n",
    "        confirmation_point_dict[p].sort(key = lambda x: -x[-1])\n",
    "\n",
    "\n",
    "        if test_point_dict[p] != confirmation_point_dict[p]:\n",
    "            print(p, test_point_dict[p])\n",
    "            print(p, confirmation_point_dict[p])\n",
    "            print('----')\n",
    "\n",
    "    assert(confirmation_point_dict == test_point_dict)\n",
    "\n",
    "print('all correct here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure that we have parameters s.t. each point has 15-30 couplings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, point_dict = make_arrangement(Jc=5*10**-9, a=0.0028, N = 10000)\n",
    "\n",
    "histogram = [0 for _ in range(10)]\n",
    "for p in points:\n",
    "    num_couplings = len(point_dict[(p[0], p[1])])\n",
    "    index=num_couplings//5\n",
    "    histogram[index]+=1\n",
    "\n",
    "xs = [str(i*5)+\"-\"+str((i+1)*5) for i in range(10)]\n",
    "\n",
    "plt.bar(xs, histogram)\n",
    "plt.title('distribution of number of couplings')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jc=5 * 10**-9, a=0.0028, N = 10000 seems to work okay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for clustering\n",
    "\n",
    "**max_coupling(points, point_dict)** This function returns the maximum coupling present, and the two points with that coupling\n",
    "\n",
    "returns: Jmax, [p1, p2]\n",
    "\n",
    "**check_cluster(cluster, point_dict)** This confirms that given a cluster (of the form (p1, p2, p3, p4)) is a cluster (i.e. all points are connected to all other points in the cluster)\n",
    "\n",
    "returns: Bool (true/False)\n",
    "\n",
    "**get_cluster_couplings(cluster, point_dict)** given a cluster, what are the couplings? \n",
    "\n",
    "returns: J01, J02, J03, J12, J13, J23\n",
    "\n",
    "**max_cluster(points, point_dict)** Now we use these functions together in max_cluster. This returns the cluster which is most strongly coupled -- that is, it has the strongest coupling in it, $J_max$, and of the clusters containing $J_{max}$, its smallest coupling is the largest. \n",
    "\n",
    "returns: cluster, cluster strengths \n",
    "\n",
    "cluster strengths are of the form: J01, J02, J03, J12, J13, J23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_cluster(points, point_dict):\n",
    "    \"\"\"\n",
    "    Find a spatially tight cluster of points\n",
    "    \"\"\"\n",
    "    #first identify the strongest coupling\n",
    "    max_J, points = max_coupling(points, point_dict)\n",
    "    \n",
    "    \"\"\"\n",
    "    get connecting points\n",
    "    \"\"\"\n",
    "    \n",
    "    connections0 = point_dict[points[0]]\n",
    "    connections1 = point_dict[points[1]]\n",
    "    \n",
    "    \"\"\"\n",
    "    now we look for connecting points between these sets of points\n",
    "    \"\"\"\n",
    "    points0, _ = zip(*connections0)\n",
    "    points1, _ = zip(*connections1)\n",
    "    \n",
    "    possible_clusters = []\n",
    "    \n",
    "    for p0 in points0:\n",
    "        for p1 in points1:\n",
    "            #check if its a possible cluster (all points are connected to one another)\n",
    "            if check_cluster((points[0], points[1], p0, p1), point_dict):\n",
    "                possible_clusters.append((points[0], points[1], p0, p1))\n",
    "    \n",
    "    \"\"\"\n",
    "    what if we have found zero clusters to use\n",
    "    Let's raise an assertion error if this occurs\n",
    "    We can deal with this as it arises\n",
    "    \"\"\"\n",
    "    assert(len(possible_clusters)>0)\n",
    "    \n",
    "    \"\"\"\n",
    "    of these possible clusters, lets choose the one with the strongest\n",
    "    couplings\n",
    "    \n",
    "    By strongest we mean that \n",
    "    min(np.abs(current_coupling)) \n",
    "    is maximized\n",
    "    \"\"\"\n",
    "    min_coupling = []\n",
    "    coupling_strengths = []\n",
    "    \n",
    "    for cluster in possible_clusters:\n",
    "        current_coupling = get_cluster_couplings(cluster, point_dict)\n",
    "        min_coupling.append(min(np.abs(current_coupling)))\n",
    "        coupling_strengths.append(current_coupling)\n",
    "    \n",
    "    max_index = np.argmax(min_coupling)\n",
    "    \n",
    "    favored_cluster = possible_clusters[max_index]\n",
    "    \n",
    "    return favored_cluster, coupling_strengths[max_index]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "cProfile.run('make_arrangement(Jc=5*10**-9, a=0.0028, N = 10000)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test max_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Jc = .01\n",
    "test_a = 1/10\n",
    "test_N = 1000\n",
    "\n",
    "test_points, test_point_dict = make_arrangement(Jc=test_Jc, a=test_a, N = test_N)\n",
    "\"\"\"\n",
    "make one connection with a huge coupling\n",
    "between p1 and p2\n",
    "\"\"\"\n",
    "Jmax = 100\n",
    "\n",
    "p1 = tuple(test_points[0])\n",
    "points1, strengths1 = zip(*test_point_dict[p1])\n",
    "\n",
    "p2 = points1[0]\n",
    "points2, strengths2 = zip(*test_point_dict[p2])\n",
    "\n",
    "strengths1, strengths2 = list(strengths1), list(strengths2)\n",
    "#now reset the coupling\n",
    "strengths1[points1.index(p2)]= Jmax\n",
    "strengths2[points2.index(p1)]= Jmax\n",
    "\n",
    "test_point_dict[p1] = sorted(zip(points1, strengths1), key = lambda x: -x[-1])\n",
    "test_point_dict[p2] = sorted(zip(points2, strengths2), key = lambda x: -x[-1])\n",
    "\n",
    "cluster, strengths = max_cluster(test_points, test_point_dict)\n",
    "\n",
    "assert(max(strengths)==Jmax)\n",
    "assert(p1 in cluster and p2 in cluster)\n",
    "print('looks good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools for RG\n",
    "\n",
    "Now we come to the tools for actually performing the rg\n",
    "\n",
    "**remove_point(points_to_remove, points, point_dict)**: This returns a new *points* and *point_dict* with the points in *points_to_remove* taken out. \n",
    "\n",
    "returns: points, point_dict\n",
    "\n",
    "**update_coupling(p1, p2, Jnew, point_dict)**: This returns a new *point_dict* with the connections between p1 and p2 updated\n",
    "\n",
    "returns: point_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_strongest_cluster(points, point_dict):\n",
    "    \"\"\"\n",
    "    given points, and a point dictionary:\n",
    "    \n",
    "    1) identify the strongest coupled cluster and its strengths\n",
    "    2) identify all points, p5, connected to the cluster\n",
    "    3) update all j35, j45 using 'renormalize couplings'\n",
    "    4) remove p1, p2\n",
    "    5) update j34. This should be done last, since the renormalized \n",
    "       j35 and j34 must be calculated using the point dict giving \n",
    "       all original couplings for the cluster\n",
    "    \n",
    "    \"\"\"\n",
    "    #identify the largest coupling\n",
    "    cluster, strengths = max_cluster(points, point_dict)\n",
    "    \n",
    "    p1, p2, p3, p4 = cluster\n",
    "    j12, j13, j14, j23, j24, j34 = strengths\n",
    "    \n",
    "    \n",
    "    #this should be j12\n",
    "    assert(j12==max(strengths))\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    now go through all connections\n",
    "    add them to a record of p5s\n",
    "    and renormalize their couplings\n",
    "    \"\"\"\n",
    "    \n",
    "    p5_dict = dict()\n",
    "    \n",
    "    for i, p in enumerate(cluster):\n",
    "        p5s, strengths_to_p5 = zip(*point_dict[p1])\n",
    "        \n",
    "        for j, p5 in enumerate(p5s):\n",
    "            \"\"\"\n",
    "            Note p5s must not be in the cluster already\n",
    "            \"\"\"\n",
    "            if p5 not in cluster:\n",
    "                if p5 not in p5_dict:\n",
    "                    #set couplings to 1,2,3,4 to be zero at first\n",
    "                    p5_dict[p5] = [0,0,0,0]\n",
    "                else:\n",
    "                    #set the ith coupling here to be ji5\n",
    "                    p5_dict[p5][i] = strengths_to_p5[j]\n",
    "\n",
    "    for p5 in p5_dict:\n",
    "        #now add the strengths of the (1,2,3,4) cluster to the front\n",
    "        #since they are necessary for getting the renormalized coupling\n",
    "        p5_dict[p5] = list(strengths)+p5_dict[p5]\n",
    "        \n",
    "        #print(p5_dict[p5])\n",
    "    \n",
    "    assert(set(p5_dict.keys()).intersection(set(cluster)) == set())\n",
    "    \"\"\"\n",
    "    Now find the updates that each coupling needs\n",
    "    \"\"\"\n",
    "    for p5 in p5_dict:\n",
    "        new_j35, new_j45 = renormalize_couplings(p5_dict[p5])\n",
    "        \n",
    "        #print('p5: ',p5)\n",
    "        #print('new couplings', new_j35, new_j45)\n",
    "        \n",
    "        #and apply these updates to obtain new point_dict\n",
    "        point_dict = update_coupling(p3, p5, new_j35, point_dict)\n",
    "        point_dict = update_coupling(p4, p5, new_j45, point_dict)\n",
    "    \n",
    "    \"\"\"\n",
    "    remove p1, p2\n",
    "    \"\"\"\n",
    "    points, point_dict = remove_point((p1, p2), points, point_dict)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    update the j34 coupling\n",
    "    \"\"\"\n",
    "    lowest_4 = get_energies(j12, j13, j14, j23, j24, j34)\n",
    "    #print('points in cluster', p1, p2, p3, p4)\n",
    "    #print(\"j12, j13, j14, j23, j24, j34\", j12, j13, j14, j23, j24, j34)\n",
    "    #print('j12 :', j12)\n",
    "    #print(lowest_4)\n",
    "    #print('predicted gap: ', j34 + (j13 - j23)*(j24 - j14)/(2*j12))\n",
    "    #print('actul gap      ', lowest_4[1] - lowest_4[0])\n",
    "    \n",
    "    newj34 = j34 + (j13 - j23)*(j24 - j14)/(2*j12)\n",
    "    \n",
    "    if newj34<0:\n",
    "        print('newj34 LOWER THAN ZERO -- this presents an issue', newj34)\n",
    "        #print(\"setting to zero \\n\")\n",
    "        #newj34=0\n",
    "        print(\"new j34 \", newj34)\n",
    "        print(\" \")\n",
    "        \n",
    "    assert(len(set(cluster))==len(cluster))\n",
    "    #assert(newj34>=0)\n",
    "    \n",
    "    point_dict = update_coupling(p3, p4, newj34, point_dict)\n",
    "    \n",
    "    \"\"\"\n",
    "    return the new setup\n",
    "    \"\"\"\n",
    "    return points, point_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test reduce_strongest_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Jc = .01\n",
    "test_a = 1/10\n",
    "test_N = 1000\n",
    "\n",
    "test_points, test_point_dict = make_arrangement(Jc=test_Jc, a=test_a, N = test_N)\n",
    "\"\"\"\n",
    "make one connection with a huge coupling\n",
    "between p1 and p2\n",
    "\"\"\"\n",
    "Jmax = 100\n",
    "\n",
    "p1 = tuple(test_points[0])\n",
    "points1, strengths1 = zip(*test_point_dict[p1])\n",
    "\n",
    "p2 = points1[0]\n",
    "points2, strengths2 = zip(*test_point_dict[p2])\n",
    "\n",
    "strengths1, strengths2 = list(strengths1), list(strengths2)\n",
    "#now reset the coupling\n",
    "strengths1[points1.index(p2)]= Jmax\n",
    "strengths2[points2.index(p1)]= Jmax\n",
    "\n",
    "test_point_dict[p1] = sorted(zip(points1, strengths1), key = lambda x: -x[-1])\n",
    "test_point_dict[p2] = sorted(zip(points2, strengths2), key = lambda x: -x[-1])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Apply cluster reduction\n",
    "\"\"\"\n",
    "\n",
    "test_points, test_point_dict = reduce_strongest_cluster(test_points, test_point_dict)\n",
    "\n",
    "\"\"\"\n",
    "lets check that p1, p2 are removed from everything\n",
    "\"\"\"\n",
    "\n",
    "for i in range(test_points.shape[0]):\n",
    "    point = tuple(test_points[i])\n",
    "    assert(point!=p1)\n",
    "    assert(point!=p2)\n",
    "\n",
    "assert(p1 not in test_point_dict)\n",
    "assert(p2 not in test_point_dict)\n",
    "\n",
    "assert(test_points.shape[0] == test_N-2)\n",
    "\n",
    "\"\"\"\n",
    "For now I'm happy with this\n",
    "I can test the couplings later\n",
    "\"\"\"\n",
    "print('appears fine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the system and running RG\n",
    "\n",
    "of course to set up an arrangement we run \n",
    "\n",
    "make_arrangement(Jc=.005, a=(1/100), N = 10000)\n",
    "\n",
    "Then to apply the rg, we apply reduce_strongest_cluster. I'll combine that into a function below called\n",
    "\n",
    "**apply_rg(points, point_dict)**\n",
    "\n",
    "This function allows us to more neatly work with the procedure. \n",
    "\n",
    "We can add analyses in the body and then iteratively apply the function to run the rg flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JXWFtOiOFy8X"
   },
   "outputs": [],
   "source": [
    "def apply_rg(points, point_dict):\n",
    "    \"\"\"\n",
    "    Apply rg once\n",
    "    \n",
    "    this is packaged so that if we want we can add in functions to track the distributions of the \n",
    "    coupling strength without modifying the functions which are actually doing the work.\n",
    "    \"\"\"\n",
    "    assert(len(points)==len(point_dict))\n",
    "    new_points, new_point_dict = reduce_strongest_cluster(points, point_dict)\n",
    "    \n",
    "    \n",
    "    couplings = []\n",
    "    for p in new_points:\n",
    "        _, strengths = zip(*new_point_dict[(p[0], p[1])])\n",
    "        couplings+= strengths\n",
    "    \n",
    "    #remove zeros\n",
    "    couplings = list(filter(lambda x: x!= 0, couplings))\n",
    "    \n",
    "    mean_of_log = np.mean(np.log(np.abs(np.array(couplings))))\n",
    "    var_of_log = np.var(np.log(np.abs(np.array(couplings))))\n",
    "    J_max, _ = max_coupling(new_points, new_point_dict)\n",
    "    \n",
    "    dist_log_width = np.std(np.log(np.abs(np.array(couplings))/J_max))\n",
    "    \n",
    "    return new_points, new_point_dict, mean_of_log, var_of_log, J_max, dist_log_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_rg(N):\n",
    "    \"\"\"\n",
    "    Apply rg iteratively N times and return various statistics of the rg flow\n",
    "    \"\"\"\n",
    "    #points, point_dict = make_arrangement(Jc=.005, a=(1/100), N = 10000)\n",
    "\n",
    "    points, point_dict = make_arrangement(Jc=5*10**-9, a=0.0028, N = 10000)\n",
    "    \n",
    "    mean_of_logs, var_of_logs, max_couplings, log_width = [], [], [], []\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        if N>100:\n",
    "            \"\"\"\n",
    "            We'll monitor the time this takes if N is large enough\n",
    "            Here I take 'large enough' to be 100\n",
    "            \"\"\"\n",
    "            if i==0:\n",
    "                initial_time = time()\n",
    "            \"\"\"\n",
    "            Here I print the remaining minutes left after every 25 iterations\n",
    "            \"\"\"\n",
    "            if i>0 and i%25==0:\n",
    "                time_elapsed = time()-initial_time\n",
    "                mean_time_per_iteration = time_elapsed/i\n",
    "                total_expected_time = mean_time_per_iteration*N\n",
    "                total_time_remaining = total_expected_time - time_elapsed\n",
    "                print(\"iteration: \", i)\n",
    "                print(\"minutes remaining: \", round(total_time_remaining/60, 2))\n",
    "                print(\"---------------------------\")\n",
    "                \n",
    "        \n",
    "        points, point_dict, mean_of_log, var_of_log, J_max, dist_log_width = apply_rg(points, point_dict)\n",
    "        \n",
    "        max_couplings.append(J_max)\n",
    "        mean_of_logs.append(mean_of_log)\n",
    "        var_of_logs.append(var_of_log)\n",
    "        log_width.append(dist_log_width)\n",
    "    \n",
    "    return mean_of_logs, var_of_logs, max_couplings, log_width\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = iterate_rg(N = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_of_logs, var_of_logs, max_couplings, log_width = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(max_couplings/max_couplings[0])\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.exp(mean_of_logs)/np.exp(mean_of_logs)[0])\n",
    "#plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.exp(mean_of_logs)/max_couplings)\n",
    "#plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(max_couplings))\n",
    "plt.plot(mean_of_logs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(np.exp(mean_of_logs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_couplings/max_couplings[0], np.sqrt(var_of_logs)/np.sqrt(var_of_logs[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Replicating Bhatt&Lee Numerically.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
